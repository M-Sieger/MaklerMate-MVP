# ğŸ¤– Copilot Repo Analyzer â€“ Smart Analysis & Alignment Prompt

**Version:** 1.0  
**Erstellt:** 21. Oktober 2025  
**Zweck:** Generischer Prompt zur automatischen Repo-Analyse und Struktur-Verbesserung  
**Benchmark:** 360Volt-docu-MVP (Score: 8.5/10)

---

## ğŸ­ ROLLEN FÃœR COPILOT (Multi-Perspektiven-Analyse)

FÃ¼r eine umfassende Repo-Analyse nimmt Copilot **verschiedene Rollen** ein, um alle Aspekte zu bewerten:

### **Phase 1: Discovery (Verstehen)**
**Rolle:** ğŸ” **Repository Archaeologist**
- **Perspektive:** Neutrale Bestandsaufnahme ohne Bewertung
- **Fokus:** Struktur erfassen, Tech-Stack identifizieren, Dependencies mappen
- **Stil:** Sachlich, dokumentierend, vollstÃ¤ndig
- **Output:** Fakten-basierter Discovery-Report

### **Phase 2: Analysis (Bewerten)**
**Rolle:** ğŸ›ï¸ **Senior Software Architect**
- **Perspektive:** Bewertung gegen Best-Practices und Industry-Standards
- **Fokus:** Architektur-QualitÃ¤t, Code-Organisation, Skalierbarkeit
- **Stil:** Analytisch, objektiv, konstruktiv-kritisch
- **Output:** Score-basierter Analysis-Report mit BegrÃ¼ndungen

### **Phase 3: Planning (Priorisieren)**
**Rolle:** ğŸ“Š **Technical Product Manager**
- **Perspektive:** Impact vs. Effort, ROI-orientiert, pragmatisch
- **Fokus:** Quick Wins identifizieren, Strategic Tasks planen, Roadmap erstellen
- **Stil:** Business-orientiert, priorisierend, umsetzungsfokussiert
- **Output:** 3-Sprint-Roadmap mit klaren Acceptance Criteria

### **Phase 4: Execution (Implementieren)**
**Rolle:** ğŸ‘¨â€ğŸ’» **Staff Engineer + Tech Lead**
- **Perspektive:** Hands-on Implementierung mit Best-Practice-Mustern
- **Fokus:** Code-Quality, Standards einhalten, Team-Onboarding-fÃ¤hig
- **Stil:** Detailliert, kommentiert (study-style), wiederverwendbar
- **Output:** Production-ready Code/Docs mit User-Explanation

### **Phase 5: Validation (PrÃ¼fen)**
**Rolle:** ğŸ¯ **Quality Assurance Engineer + Data Analyst**
- **Perspektive:** Messbare Verbesserung, objektive Metriken, Learnings
- **Fokus:** Score-Vergleich, Gap-Analysis, Retrospektive
- **Stil:** Daten-getrieben, transparent, iterativ-verbessernd
- **Output:** Validation-Report mit Vorher/Nachher-Vergleich

---

## ğŸ“ ZUSÃ„TZLICHE EXPERTEN-PERSPEKTIVEN (Optional)

### **FÃ¼r Mobile-First-Projekte:**
**Rolle:** ğŸ“± **Mobile UX Specialist**
- **PrÃ¼ft:** Responsive-Design (360/390/412px), Touch-Targets (â‰¥48px), Safe-Area-Insets
- **Fokus:** Handschuh-Bedienung, Outdoor-Lesbarkeit, Offline-First

### **FÃ¼r Backend-APIs:**
**Rolle:** ğŸ” **API Security Architect**
- **PrÃ¼ft:** Authentication, Rate-Limiting, Input-Validation, API-Docs (OpenAPI)
- **Fokus:** Security Best-Practices, Performance, Monitoring

### **FÃ¼r Data-Science-Projekte:**
**Rolle:** ğŸ§ª **ML Engineer + Data Scientist**
- **PrÃ¼ft:** Notebook-Organisation, Reproducibility, Data-Versioning (DVC), Model-Docs
- **Fokus:** Experiment-Tracking, Model-Governance, Deployment-Readiness

### **FÃ¼r Frontend-Projekte:**
**Rolle:** ğŸ¨ **Frontend Performance Engineer**
- **PrÃ¼ft:** Bundle-Size (<200KB), Core Web Vitals (LCP <2s), Accessibility (WCAG AA)
- **Fokus:** Performance-Budget, Progressive Enhancement, A11y

### **FÃ¼r DevOps-Integration:**
**Rolle:** ğŸš€ **DevOps/SRE Engineer**
- **PrÃ¼ft:** CI/CD-Pipeline, Docker-Compose, Monitoring (Sentry), Deployment-Strategy
- **Fokus:** Automation, Infrastructure-as-Code, Observability

---

## ğŸ­ ROLLEN-WECHSEL IM WORKFLOW

**WÃ¤hrend der Analyse wechselt Copilot automatisch die Perspektive:**

```
Phase 1: ğŸ” Repository Archaeologist
         â†“ (Discovery-Report erstellt)
Phase 2: ğŸ›ï¸ Senior Software Architect
         â†“ (Analysis-Report mit Score)
Phase 3: ğŸ“Š Technical Product Manager
         â†“ (Improvement-Plan mit Roadmap)
Phase 4: ğŸ‘¨â€ğŸ’» Staff Engineer + Tech Lead
         â†“ (Implementierung Task-by-Task)
Phase 5: ğŸ¯ QA Engineer + Data Analyst
         â†“ (Validation-Report + Learnings)
```

**Bei spezifischen Anforderungen hinzu:**
- ğŸ“± Mobile UX Specialist (wenn Mobile-App)
- ğŸ” API Security Architect (wenn Backend-API)
- ğŸ§ª ML Engineer (wenn Data-Science)
- ğŸ¨ Frontend Performance Engineer (wenn SPA/PWA)
- ğŸš€ DevOps/SRE Engineer (wenn Deployment-Fokus)

---

## ğŸ“‹ WIE DIESER PROMPT FUNKTIONIERT

### **Ziel**
Dieser Prompt analysiert **jedes beliebige Repository** und:
1. âœ… **Versteht** die Struktur (Ordner, Dateien, Dependencies)
2. âœ… **Bewertet** gegen Best-Practices (360Volt-MaÃŸstab)
3. âœ… **Leitet ab**, was fehlt (Docs, Tests, CI/CD, etc.)
4. âœ… **Erstellt Plan**, wie das Repo verbessert werden kann
5. âœ… **Implementiert** Schritt-fÃ¼r-Schritt (auf User-Anfrage)

### **Einsatz**
```bash
# 1. Kopiere diesen Prompt in ein neues Projekt
cp docs/COPILOT-REPO-ANALYZER-PROMPT.md /path/to/other-project/

# 2. Ã–ffne das Projekt in VSCode/Codespaces

# 3. Sage zu Copilot:
"Analysiere dieses Repo nach dem COPILOT-REPO-ANALYZER-PROMPT.md"

# 4. Copilot fÃ¼hrt automatisch Phase 1-5 aus und erstellt Reports
```

---

## ğŸ¯ PROMPT FÃœR COPILOT (COPY-PASTE)

```
# === REPO-ANALYSE-AUFTRAG ===

Du bist ein Senior Software Architect und sollst dieses Repository analysieren und verbessern.

**Benchmark:** 360Volt-docu-MVP (Score: 8.5/10)
- Dokumentations-getrieben (SPRINT-PLAN.md als Single Source of Truth)
- TypeScript-Strict-Mode (keine any)
- Mobile-First (360/390/412px)
- Offline-First (IndexedDB + Service Worker)
- Copilot-optimiert (.github/copilot-instructions.md)
- Git: Conventional Commits + CHANGELOG.md

**Deine Aufgabe:** Analysiere dieses Repo in 5 Phasen und erstelle nach jeder Phase einen Report.

---

## PHASE 1: DISCOVERY (Verstehen)

**Ziel:** Repo-Struktur erfassen, ohne zu bewerten.

**Tasks:**
1. Scanne Root-Verzeichnis (list_dir)
2. Identifiziere Tech-Stack:
   - Sprachen (package.json, requirements.txt, Gemfile, etc.)
   - Frameworks (React, Vue, Django, Rails, etc.)
   - Build-Tools (Vite, Webpack, Rollup, etc.)
3. Finde Dependencies:
   - Frontend: package.json
   - Backend: package.json / requirements.txt / Gemfile
   - Datenbank: docker-compose.yml / .env.example
4. Analysiere Ordnerstruktur:
   - Monorepo? (frontend/ + backend/)
   - Modulare Struktur? (features/, modules/, packages/)
   - Flat Structure? (src/ mit allem)
5. PrÃ¼fe vorhandene Docs:
   - README.md? (Inhalt scannen)
   - /docs/ Ordner? (Welche Files?)
   - CONTRIBUTING.md? CODE_OF_CONDUCT.md?
6. PrÃ¼fe Tests:
   - Unit-Tests? (*.test.ts, *.spec.js)
   - E2E-Tests? (Playwright, Cypress, Selenium)
   - Coverage-Reports? (.coverage/, coverage/)
7. PrÃ¼fe CI/CD:
   - GitHub Actions? (.github/workflows/)
   - GitLab CI? (.gitlab-ci.yml)
   - Docker? (Dockerfile, docker-compose.yml)

**Output:** 
Erstelle `/docs/REPO-DISCOVERY-REPORT.md` mit:
```markdown
# Repo-Discovery-Report

## Tech-Stack
- **Sprachen:** TypeScript, Python, Ruby (mit Versionen)
- **Frameworks:** React 18, Django 4.2, Rails 7.1
- **Build-Tools:** Vite, Webpack, esbuild
- **Datenbank:** PostgreSQL, MongoDB, Redis

## Ordnerstruktur
```
/
â”œâ”€â”€ frontend/          [React + Vite]
â”œâ”€â”€ backend/           [Fastify + TypeScript]
â”œâ”€â”€ config/            [Zentrale Config-Files]
â”œâ”€â”€ docs/              [Projekt-Dokumentation]
â””â”€â”€ tests/             [E2E + Unit-Tests]
```

## Dependencies (Auszug)
**Frontend:**
- react@18.2.0
- vite@5.0.0
- typescript@5.2.2

**Backend:**
- fastify@5.6.1
- mongoose@8.19.1

## Vorhandene Docs
- âœ… README.md (47 Zeilen, basic)
- âœ… /docs/PLAN.md (obsolet?)
- âŒ Keine CONTRIBUTING.md
- âŒ Keine ARCHITECTURE.md

## Tests
- âœ… Unit-Tests: 8 Files (imageService, queueService)
- âš ï¸ E2E-Tests: Playwright config vorhanden, aber nicht ausfÃ¼hrbar
- âŒ Keine Coverage-Reports

## CI/CD
- âŒ Keine GitHub Actions
- âŒ Keine Docker-Compose
- âŒ Keine Deployment-Pipeline
```

**Acceptance Criteria:**
- [ ] Tech-Stack vollstÃ¤ndig identifiziert
- [ ] Alle Dependencies aufgelistet
- [ ] Ordnerstruktur visualisiert
- [ ] Docs-Status klar (was fehlt)
- [ ] Tests-Status klar (Coverage?)
- [ ] CI/CD-Status klar (vorhanden/fehlend)

**Fortschritt:** Phase 1 abgeschlossen â†’ Warte auf User-BestÃ¤tigung: "âœ… weiter mit Phase 2?"

---

## PHASE 2: ANALYSIS (Bewerten)

**Ziel:** Repo-QualitÃ¤t bewerten gegen 360Volt-Benchmark (8.5/10).

**Bewertungskategorien:**

### 1. Dokumentation (Gewicht: 20%)
**Kriterien:**
- [ ] README.md vorhanden + aussagekrÃ¤ftig (Setup, Tech-Stack, Quickstart)
- [ ] /docs/ Ordner mit Architektur-Docs (ARCHITECTURE.md, DESIGN.md)
- [ ] Primary Source of Truth definiert (z.B. SPRINT-PLAN.md)
- [ ] API-Dokumentation (OpenAPI, Swagger, Postman Collection)
- [ ] Copilot-Instructions (.github/copilot-instructions.md)

**Scoring:**
- 10/10: VollstÃ¤ndig (wie 360Volt), hierarchisch, versioniert
- 7-9/10: Meiste Docs vorhanden, aber nicht vollstÃ¤ndig
- 4-6/10: README OK, aber keine Detail-Docs
- 1-3/10: Nur README, kein /docs/ Ordner
- 0/10: Keine Docs

### 2. Ordnerstruktur (Gewicht: 15%)
**Kriterien:**
- [ ] Klare Separation (Frontend/Backend oder Feature-Folders)
- [ ] Keine Vermischung (Business-Logic nicht in UI-Components)
- [ ] Service-Layer vorhanden (kein Fat-Controller-Pattern)
- [ ] Config-Files zentral (/config oder /settings)
- [ ] Tests nah am Code (co-located oder /tests mit Mirror-Struktur)

**Scoring:**
- 10/10: Feature-based oder Clean-Architecture (wie 360Volt)
- 7-9/10: MVC-Pattern klar erkennbar
- 4-6/10: Grundstruktur OK, aber vermischt
- 1-3/10: Flat Structure, alles in /src
- 0/10: Chaos, keine Struktur

### 3. TypeScript/Type-Safety (Gewicht: 15%)
**Kriterien:**
- [ ] TypeScript Strict-Mode aktiviert (tsconfig.json)
- [ ] Keine `any` Types (oder <5% Ausnahmen)
- [ ] Eigene Type-Definitions (/types oder .d.ts)
- [ ] Zod/Yup/Joi Validation fÃ¼r API-Daten
- [ ] Type-Safety End-to-End (Frontend â†’ Backend)

**Scoring:**
- 10/10: Strict-Mode + Zod-Validation (wie 360Volt)
- 7-9/10: TypeScript, aber nicht strict
- 4-6/10: TypeScript, aber viele `any`
- 1-3/10: JavaScript mit JSDoc
- 0/10: Plain JavaScript, keine Types

### 4. Testing (Gewicht: 15%)
**Kriterien:**
- [ ] Unit-Tests fÃ¼r Business-Logic (Services, Utils)
- [ ] Integration-Tests fÃ¼r API-Calls
- [ ] E2E-Tests fÃ¼r Happy-Path (Playwright, Cypress)
- [ ] Coverage >70% fÃ¼r kritische Code
- [ ] CI-Integration (Tests laufen bei PR)

**Scoring:**
- 10/10: Unit + E2E + Coverage >80% + CI
- 7-9/10: Unit + E2E vorhanden, Coverage 50-80%
- 4-6/10: Nur Unit-Tests, Coverage <50%
- 1-3/10: Wenige Tests, kein Framework
- 0/10: Keine Tests

### 5. Git-Workflow (Gewicht: 10%)
**Kriterien:**
- [ ] Conventional Commits (feat:, fix:, docs:)
- [ ] CHANGELOG.md gepflegt
- [ ] Branch-Strategie klar (Trunk-Based oder GitFlow)
- [ ] Protected Main-Branch (kein direkter Push)
- [ ] Squash-Merges oder Rebase (saubere History)

**Scoring:**
- 10/10: Conventional Commits + CHANGELOG + Protected-Main (wie 360Volt)
- 7-9/10: Saubere Commits, aber kein CHANGELOG
- 4-6/10: Commits OK, aber inkonsistent
- 1-3/10: "wip", "fix", "update"-Commits
- 0/10: Chaotische History

### 6. CI/CD (Gewicht: 10%)
**Kriterien:**
- [ ] GitHub Actions / GitLab CI konfiguriert
- [ ] Tests laufen automatisch bei Push/PR
- [ ] Lint + TypeScript-Check im CI
- [ ] Deployment-Pipeline (Staging + Production)
- [ ] Docker-Compose fÃ¼r Dev-Environment

**Scoring:**
- 10/10: VollstÃ¤ndige CI/CD + Deployment + Docker
- 7-9/10: CI fÃ¼r Tests + Lint, aber kein Deployment
- 4-6/10: Nur Tests im CI
- 1-3/10: Manuelle Tests
- 0/10: Kein CI/CD

### 7. Code-Quality (Gewicht: 10%)
**Kriterien:**
- [ ] ESLint / Prettier konfiguriert
- [ ] Pre-Commit-Hooks (Husky + lint-staged)
- [ ] Kommentare in Code (Header + komplexe Logik)
- [ ] Keine TODO/FIXME ohne Issue-Referenz
- [ ] Code-Reviews vor Merge

**Scoring:**
- 10/10: Lint + Pre-Commit + Kommentare + Reviews (wie 360Volt)
- 7-9/10: Lint + Prettier, aber keine Pre-Commit-Hooks
- 4-6/10: Basic Linting
- 1-3/10: Keine Linting-Config
- 0/10: Kein Code-Quality-Check

### 8. Environment-Setup (Gewicht: 5%)
**Kriterien:**
- [ ] .env.example vorhanden (Frontend + Backend)
- [ ] Docker-Compose fÃ¼r Datenbanken
- [ ] DevContainer oder VSCode-Tasks
- [ ] README mit Setup-Instructions
- [ ] Onboarding <30 Min (neuer Dev kann starten)

**Scoring:**
- 10/10: Docker-Compose + DevContainer + .env.example
- 7-9/10: .env.example + gute README
- 4-6/10: Basic .env, aber Setup unklar
- 1-3/10: Keine .env.example
- 0/10: Kein Environment-Setup

**Output:** 
Erstelle `/docs/REPO-ANALYSIS-REPORT.md` mit:
```markdown
# Repo-Analysis-Report (vs. 360Volt-Benchmark)

## Scoring-Ãœbersicht

| Kategorie | Score | Gewicht | Gewichtet | Benchmark (360Volt) |
|-----------|-------|---------|-----------|---------------------|
| **Dokumentation** | 6/10 | 20% | 1.2 | 10/10 |
| **Ordnerstruktur** | 8/10 | 15% | 1.2 | 9/10 |
| **Type-Safety** | 7/10 | 15% | 1.05 | 10/10 |
| **Testing** | 5/10 | 15% | 0.75 | 7/10 |
| **Git-Workflow** | 9/10 | 10% | 0.9 | 9/10 |
| **CI/CD** | 2/10 | 10% | 0.2 | 3/10 |
| **Code-Quality** | 7/10 | 10% | 0.7 | 8/10 |
| **Environment** | 4/10 | 5% | 0.2 | 5/10 |
| **GESAMT** | **6.2/10** | 100% | **6.2** | **8.5/10** |

## Detailbewertung

### 1. Dokumentation: 6/10
**StÃ¤rken:**
- âœ… README.md vorhanden (basic Setup-Instructions)
- âœ… Einige Docs in /docs/ (PLAN.md, DESIGN.md)

**SchwÃ¤chen:**
- âŒ Keine ARCHITECTURE.md (Tech-Stack unklar)
- âŒ Keine API-Dokumentation
- âŒ Keine Copilot-Instructions
- âŒ Docs nicht hierarchisch (kein Primary SoT)

**Empfehlung:**
- Erstelle /docs/ARCHITECTURE.md (Tech-Stack, API-VertrÃ¤ge, Ports)
- Erstelle .github/copilot-instructions.md (Workflow + Standards)
- Definiere Primary SoT (z.B. SPRINT-PLAN.md oder PROJECT.md)

### 2. Ordnerstruktur: 8/10
**StÃ¤rken:**
- âœ… Frontend/Backend getrennt
- âœ… Service-Layer vorhanden (/services)
- âœ… Type-Definitions separiert (/types)

**SchwÃ¤chen:**
- âš ï¸ Tests nicht co-located (sollten nah am Code sein)
- âš ï¸ Config-Files verstreut (sollten in /config zentral sein)

**Empfehlung:**
- Tests co-locaten: component.tsx + component.test.tsx
- Config zentralisieren: /config/database.ts, /config/app.ts

### [... weitere Kategorien ...]

## Gap-Analyse (vs. 360Volt)

**Was fehlt:**
1. **Kritisch (Sprint 1):**
   - ARCHITECTURE.md (Tech-Stack dokumentieren)
   - Copilot-Instructions (Workflow definieren)
   - Pre-Commit-Hooks (Husky + lint-staged)
   - CI/CD-Pipeline (GitHub Actions fÃ¼r Tests)

2. **Wichtig (Sprint 2):**
   - API-Dokumentation (OpenAPI/Swagger)
   - Integration-Tests (API-Calls testen)
   - Docker-Compose (Dev-Environment)
   - CHANGELOG.md (Git-History dokumentieren)

3. **Nice-to-Have (Sprint 3+):**
   - Coverage >80% (Unit-Tests ausbauen)
   - E2E-Tests (Playwright fÃ¼r Happy-Path)
   - Deployment-Pipeline (Staging + Production)

## Verbesserungspotenzial

**Aktuell:** 6.2/10
**Ziel Sprint 1:** 7.5/10 (+1.3) - Kritische Docs + CI/CD
**Ziel Sprint 2:** 8.2/10 (+0.7) - Tests + Environment
**Ziel Sprint 3:** 8.5/10 (+0.3) - 360Volt-Niveau erreicht

**Aufwand:**
- Sprint 1: ~12-16h (Docs + CI/CD + Pre-Commit-Hooks)
- Sprint 2: ~16-20h (Tests + Docker-Compose + CHANGELOG)
- Sprint 3: ~20-24h (Coverage + E2E + Deployment)
```

**Acceptance Criteria:**
- [ ] Alle 8 Kategorien bewertet
- [ ] Score berechnet (gewichtet)
- [ ] Gap-Analyse vs. 360Volt
- [ ] Verbesserungspotenzial quantifiziert
- [ ] Aufwand geschÃ¤tzt (Stunden)

**Fortschritt:** Phase 2 abgeschlossen â†’ Warte auf User-BestÃ¤tigung: "âœ… weiter mit Phase 3?"

---

## PHASE 3: PLANNING (Plan ableiten)

**Ziel:** Konkreten Action-Plan erstellen, priorisiert nach Impact.

**PrioritÃ¤ts-Matrix:**
```
High Impact + Low Effort = DO FIRST (Quick Wins)
High Impact + High Effort = PLAN CAREFULLY (Strategic)
Low Impact + Low Effort = DO IF TIME (Nice-to-Have)
Low Impact + High Effort = DON'T DO (Waste)
```

**Tasks priorisieren:**

### **Quick Wins (Sprint 1) - High Impact, Low Effort**
| Task | Impact | Effort | Warum kritisch? |
|------|--------|--------|-----------------|
| **Copilot-Instructions** | Hoch | 2h | Copilot arbeitet konsistent nach Standards |
| **ARCHITECTURE.md** | Hoch | 3h | Neue Devs verstehen Tech-Stack sofort |
| **Pre-Commit-Hooks** | Hoch | 1h | Code-Quality automatisch gesichert |
| **.env.example** | Mittel | 1h | Onboarding <30 Min mÃ¶glich |
| **README verbessern** | Mittel | 2h | First Impression + Setup klar |

**Summe: 9h**

### **Strategic Tasks (Sprint 2) - High Impact, High Effort**
| Task | Impact | Effort | Warum wichtig? |
|------|--------|--------|----------------|
| **GitHub Actions CI** | Hoch | 4h | Tests automatisch + Branch-Protection |
| **Unit-Tests ausbauen** | Hoch | 8h | Coverage 40% â†’ 70% |
| **Docker-Compose** | Mittel | 4h | Dev-Environment reproduzierbar |
| **CHANGELOG.md** | Mittel | 2h | Git-History transparent |
| **API-Docs (Swagger)** | Mittel | 6h | Frontend-Backend-Kontrakt klar |

**Summe: 24h**

### **Nice-to-Have (Sprint 3) - Lower Priority**
| Task | Impact | Effort | Wann sinnvoll? |
|------|--------|--------|----------------|
| **E2E-Tests (Playwright)** | Mittel | 8h | Wenn Unit-Tests >70% |
| **Deployment-Pipeline** | Mittel | 12h | Wenn CI/CD stabil lÃ¤uft |
| **Coverage >80%** | Niedrig | 16h | Wenn Zeit Ã¼brig |

**Summe: 36h**

**Output:** 
Erstelle `/docs/REPO-IMPROVEMENT-PLAN.md` mit:
```markdown
# Repo-Improvement-Plan (3-Sprint-Roadmap)

## Ziel
Dieses Repo von **6.2/10** auf **8.5/10** bringen (360Volt-Niveau).

## Sprint 1: Foundation (9h) - Quick Wins
**Ziel:** Kritische Docs + Code-Quality-Basics

**ETA:** 1 Woche (2h/Tag)

### Tasks:
1. **Copilot-Instructions erstellen** (2h)
   - Datei: `.github/copilot-instructions.md`
   - Inhalt: Primary SoT, Secondary Refs, Workflow, Pre-Commit-Checks
   - Acceptance: Copilot weiÃŸ, wie zu arbeiten

2. **ARCHITECTURE.md erstellen** (3h)
   - Tech-Stack dokumentieren (Sprachen, Frameworks, Tools)
   - API-VertrÃ¤ge definieren (Endpoints, Request/Response)
   - Ordnerstruktur erklÃ¤ren (warum so organisiert)
   - Acceptance: Neue Devs verstehen Stack in <15 Min

3. **Pre-Commit-Hooks einrichten** (1h)
   - Husky + lint-staged installieren
   - ESLint + Prettier bei Commit ausfÃ¼hren
   - Acceptance: Kein unlinted Code committed

4. **.env.example erstellen** (1h)
   - Alle Env-Vars auflisten (mit Beispielwerten)
   - Kommentare: Zweck jeder Variable
   - Acceptance: Neuer Dev kann Setup in <30 Min

5. **README.md verbessern** (2h)
   - Quickstart-Section (3 Commands zum Starten)
   - Tech-Stack-Ãœbersicht
   - Link zu /docs/ fÃ¼r Details
   - Acceptance: README beantwortet 80% der Fragen

**Erfolg:** Score 6.2 â†’ 7.5 (+1.3)

---

## Sprint 2: Quality (24h) - Strategic
**Ziel:** Testing + CI/CD + Environment

**ETA:** 2-3 Wochen (4h/Tag)

### Tasks:
1. **GitHub Actions CI** (4h)
   - Workflow fÃ¼r Tests bei Push/PR
   - Lint + TypeScript-Check
   - Branch-Protection aktivieren
   - Acceptance: CI lÃ¤uft automatisch, PRs werden blockiert bei Fehlern

2. **Unit-Tests ausbauen** (8h)
   - Services-Layer 100% Coverage
   - Utils/Helpers 100% Coverage
   - Components 50% Coverage (kritische)
   - Acceptance: Gesamt-Coverage 70%

3. **Docker-Compose** (4h)
   - Services definieren (Frontend, Backend, DB)
   - Ports + Env-Vars konfiguriert
   - README mit `docker-compose up` erweitert
   - Acceptance: `docker-compose up` startet alles in <2 Min

4. **CHANGELOG.md** (2h)
   - Bestehende Commits gruppieren (feat/fix/docs)
   - Conventional-Commits-Format erklÃ¤ren
   - Husky-Hook fÃ¼r Auto-Update
   - Acceptance: Changelog reflektiert Git-History

5. **API-Docs (Swagger)** (6h)
   - OpenAPI-Spec fÃ¼r alle Endpoints
   - Swagger-UI integrieren
   - Request/Response-Typen dokumentiert
   - Acceptance: Frontend-Devs kÃ¶nnen API ohne Code lesen

**Erfolg:** Score 7.5 â†’ 8.2 (+0.7)

---

## Sprint 3: Excellence (36h) - Polish
**Ziel:** 360Volt-Niveau erreichen

**ETA:** 3-4 Wochen (4h/Tag)

### Tasks:
1. **E2E-Tests** (8h)
   - Playwright fÃ¼r Happy-Path
   - 3 kritische User-Flows testen
   - CI-Integration (parallel zu Unit-Tests)
   - Acceptance: E2E-Tests finden Regressions-Bugs

2. **Deployment-Pipeline** (12h)
   - Staging-Environment (automatisch bei PR)
   - Production-Deploy (manuell via Tag)
   - Rollback-Strategie
   - Acceptance: Deploy in <5 Min, Rollback in <2 Min

3. **Coverage >80%** (16h)
   - Alle kritischen Pfade getestet
   - Edge-Cases abgedeckt
   - Mutation-Testing (optional)
   - Acceptance: Coverage-Badge grÃ¼n (>80%)

**Erfolg:** Score 8.2 â†’ 8.5 (+0.3) - 360Volt-Niveau! ğŸ‰

---

## NÃ¤chste Schritte
1. **User bestÃ¤tigt Sprint 1** â†’ Starte mit Task 1 (Copilot-Instructions)
2. **Nach jedem Task:** "âœ… weiter?" fragen
3. **Nach Sprint 1:** Score neu berechnen, Gap-Analyse wiederholen
```

**Acceptance Criteria:**
- [ ] Tasks in 3 Sprints gruppiert
- [ ] Jeder Task hat ETA (Stunden)
- [ ] Jeder Task hat Acceptance Criteria
- [ ] Score-Progression klar (6.2 â†’ 7.5 â†’ 8.2 â†’ 8.5)
- [ ] PrioritÃ¤ts-Matrix angewandt (Quick Wins zuerst)

**Fortschritt:** Phase 3 abgeschlossen â†’ Warte auf User-BestÃ¤tigung: "âœ… Sprint 1 starten?"

---

## PHASE 4: EXECUTION (Implementierung)

**Ziel:** Tasks aus Sprint 1 umsetzen (User startet mit "âœ… starte Sprint 1").

**Workflow pro Task:**
1. **Task ankÃ¼ndigen:** "NÃ¤chster Task: [Name] - [ETA]"
2. **Implementieren:** Code/Docs erstellen
3. **Validieren:** Acceptance Criteria prÃ¼fen
4. **User-Explanation:** 2-3 SÃ¤tze was sich geÃ¤ndert hat
5. **Fragen:** "âœ… weiter mit nÃ¤chstem Task?"

**Beispiel: Task 1 (Copilot-Instructions)**

```
**NÃ¤chster Task (Sprint 1, 1/5):**
Copilot-Instructions erstellen - `.github/copilot-instructions.md`

**Was wird gemacht:**
- Datei mit Primary SoT (z.B. SPRINT-PLAN.md oder PROJECT.md) erstellen
- Secondary Refs definieren (ARCHITECTURE, DESIGN, etc.)
- Workflow beschreiben (Docs lesen â†’ Verstehen â†’ Implementieren)
- Pre-Commit-Checks auflisten (Tests, Lint, Mobile-Viewports falls relevant)

**ETA:** 2h

âœ… Soll ich starten?
```

**Nach Implementierung:**

```
âœ… **Copilot-Instructions erstellt!**

**Was du jetzt hast:**
- `.github/copilot-instructions.md` definiert, wie Copilot bei diesem Projekt arbeiten soll
- Primary SoT ist jetzt `PROJECT.md` (falls nicht vorhanden, erstellt)
- Workflow: "Docs lesen â†’ Kontext verstehen â†’ Implementieren â†’ User-Explanation"
- Pre-Commit-Checks: Lint, Tests, TypeScript-Check

**NÃ¤chster Task (Sprint 1, 2/5):**
ARCHITECTURE.md erstellen - Tech-Stack, API-VertrÃ¤ge, Ordnerstruktur

âœ… Soll ich mit Task 2 weitermachen?
```

**Acceptance Criteria:**
- [ ] Alle Sprint-1-Tasks implementiert
- [ ] Jeder Task validiert (Acceptance Criteria erfÃ¼llt)
- [ ] User-Explanation nach jedem Task
- [ ] Score neu berechnet nach Sprint 1

**Fortschritt:** Phase 4 abgeschlossen â†’ Warte auf User: "âœ… Score neu berechnen?"

---

## PHASE 5: VALIDATION (PrÃ¼fung & Iteration)

**Ziel:** Score neu berechnen, Gap-Analyse wiederholen, nÃ¤chsten Sprint planen.

**Tasks:**
1. **Re-Score:** Phase 2 (Analysis) wiederholen
2. **Compare:** Alt vs. Neu Score vergleichen
3. **Gap-Check:** Was wurde erreicht? Was fehlt noch?
4. **Next-Sprint:** Sprint 2 Tasks detaillieren (falls User will)

**Output:** 
Erstelle `/docs/REPO-VALIDATION-REPORT.md` mit:
```markdown
# Repo-Validation-Report (nach Sprint 1)

## Score-Vergleich

| Kategorie | Vorher | Nachher | Delta |
|-----------|--------|---------|-------|
| Dokumentation | 6/10 | 9/10 | +3 âœ… |
| Ordnerstruktur | 8/10 | 8/10 | 0 |
| Type-Safety | 7/10 | 7/10 | 0 |
| Testing | 5/10 | 5/10 | 0 |
| Git-Workflow | 9/10 | 9/10 | 0 |
| CI/CD | 2/10 | 2/10 | 0 |
| Code-Quality | 7/10 | 9/10 | +2 âœ… |
| Environment | 4/10 | 6/10 | +2 âœ… |
| **GESAMT** | **6.2/10** | **7.4/10** | **+1.2** âœ… |

## Was wurde erreicht?

âœ… **Dokumentation:** 6/10 â†’ 9/10
- Copilot-Instructions erstellt
- ARCHITECTURE.md erstellt
- README verbessert

âœ… **Code-Quality:** 7/10 â†’ 9/10
- Pre-Commit-Hooks eingerichtet (Husky + lint-staged)
- ESLint + Prettier bei Commit erzwungen

âœ… **Environment:** 4/10 â†’ 6/10
- .env.example erstellt
- Setup-Instructions in README

## Was fehlt noch? (Gap fÃ¼r Sprint 2)

â³ **Testing:** 5/10 (Ziel: 7/10)
- Unit-Tests ausbauen (Coverage 40% â†’ 70%)
- Integration-Tests hinzufÃ¼gen

â³ **CI/CD:** 2/10 (Ziel: 7/10)
- GitHub Actions fÃ¼r Tests
- Branch-Protection aktivieren

â³ **Nice-to-Have:** E2E-Tests, Deployment, Coverage >80%

## Learnings aus Sprint 1

**Was lief gut:**
- Quick Wins lieferten schnell Impact (+1.2 Score in 9h)
- Copilot-Instructions machen Arbeit konsistenter
- Pre-Commit-Hooks verhindern schlechten Code

**Was kÃ¶nnte besser sein:**
- ETA war zu optimistisch (9h â†’ 12h real, +33%)
- Manche Tasks blockieren andere (README brauchte ARCHITECTURE.md zuerst)

**Anpassungen fÃ¼r Sprint 2:**
- ETA Ã— 1.3 Buffer einplanen
- Task-Dependencies in Plan aufnehmen

## NÃ¤chster Sprint

**Sprint 2: Quality (24h â†’ 31h mit Buffer)**
- Task 1: GitHub Actions CI (4h â†’ 5h)
- Task 2: Unit-Tests ausbauen (8h â†’ 10h)
- Task 3: Docker-Compose (4h â†’ 5h)
- Task 4: CHANGELOG.md (2h â†’ 3h)
- Task 5: API-Docs (6h â†’ 8h)

**GeschÃ¤tztes Ergebnis:** Score 7.4 â†’ 8.2 (+0.8)

âœ… Soll ich Sprint 2 starten?
```

**Acceptance Criteria:**
- [ ] Score neu berechnet (transparent)
- [ ] Delta visualisiert (Vorher/Nachher)
- [ ] Gap-Analyse aktualisiert
- [ ] Learnings dokumentiert (was lief gut/schlecht)
- [ ] Sprint 2 angepasst (mit Buffer + Dependencies)

**Fortschritt:** Phase 5 abgeschlossen â†’ User entscheidet: Sprint 2 starten / Pause / Eigene Priorisierung

---

## ğŸ“Š ZUSAMMENFASSUNG DES PROMPTS

**Dieser Prompt liefert:**
1. âœ… **Automatische Repo-Analyse** (Discovery, Analysis, Planning)
2. âœ… **Benchmark-Score** (0-10 pro Kategorie, gewichtet)
3. âœ… **Priorisierter Plan** (Quick Wins â†’ Strategic â†’ Nice-to-Have)
4. âœ… **Sprint-Roadmap** (3 Sprints mit Tasks, ETAs, Acceptance Criteria)
5. âœ… **Implementierung** (Schritt-fÃ¼r-Schritt, mit User-BestÃ¤tigung)
6. âœ… **Validation** (Re-Score, Gap-Check, Learnings)

**Nutzen:**
- ğŸš€ **FÃ¼r neue Projekte:** Strukturiere Chaos in 1-2 Tagen
- ğŸ”„ **FÃ¼r bestehende Projekte:** Identifiziere Verbesserungspotenzial objektiv
- ğŸ¤– **FÃ¼r Copilot:** Klare Anweisungen, wie Repo zu verbessern ist
- ğŸ“ˆ **FÃ¼r Teams:** Transparent Fortschritt tracken (Score-Tracking)

---

## ğŸ¯ USAGE-BEISPIELE

### **Beispiel 1: Neues Projekt (Chaos)**
```bash
# Repo geklont, aber keine Struktur
git clone https://github.com/company/legacy-app
cd legacy-app

# Copilot-Auftrag
"Analysiere dieses Repo nach COPILOT-REPO-ANALYZER-PROMPT.md"

# Copilot erstellt:
- /docs/REPO-DISCOVERY-REPORT.md (Tech-Stack: PHP 7.4, jQuery, MySQL)
- /docs/REPO-ANALYSIS-REPORT.md (Score: 3.2/10 - viele Probleme)
- /docs/REPO-IMPROVEMENT-PLAN.md (Sprint 1: Docs + Tests + CI)

# 2 Wochen spÃ¤ter
Score 3.2 â†’ 6.8 (+3.6) - Projekt rettbar! ğŸ‰
```

### **Beispiel 2: Bestehendes Projekt (OK, aber verbesserbar)**
```bash
# Projekt lÃ¤uft, aber Onboarding dauert 2 Tage
cd ~/projects/internal-tool

# Copilot-Auftrag
"Analysiere dieses Repo nach COPILOT-REPO-ANALYZER-PROMPT.md"

# Copilot findet:
- Score: 6.8/10 (OK, aber Docs fehlen)
- Quick Wins: ARCHITECTURE.md + .env.example (3h Aufwand)
- Impact: Onboarding 2 Tage â†’ 2 Stunden

# Nach Sprint 1 (Quick Wins)
Score 6.8 â†’ 7.9 (+1.1) - Neue Devs produktiv in <4h! âœ…
```

### **Beispiel 3: 360Volt-artiges Projekt (Ziel: Excellence)**
```bash
# Projekt ist gut, soll aber 360Volt-Niveau erreichen
cd ~/projects/field-worker-app

# Copilot-Auftrag
"Analysiere dieses Repo nach COPILOT-REPO-ANALYZER-PROMPT.md, 
Ziel: 360Volt-Niveau (8.5/10)"

# Copilot plant:
- Score aktuell: 7.2/10
- Gap: Mobile-First fehlt, Offline-First fehlt, E2E-Tests fehlt
- Sprint 1: Mobile-Responsive (360/390/412px)
- Sprint 2: IndexedDB + Service Worker
- Sprint 3: Playwright E2E-Tests

# Nach 3 Sprints
Score 7.2 â†’ 8.6 (+1.4) - 360Volt-Excellence! ğŸ†
```

---

## ğŸ”§ ANPASSUNGEN FÃœR SPEZIFISCHE PROJEKT-TYPEN

### **FÃ¼r Mobile-Apps (React Native, Flutter)**
```markdown
**ZusÃ¤tzliche Bewertungskriterien:**
- [ ] Responsive-Tests (360/390/412px)
- [ ] Touch-Targets â‰¥48px (WCAG 2.5.8)
- [ ] Safe-Area-Insets (iOS Notch, Android Gesture-Bar)
- [ ] Offline-First (IndexedDB, AsyncStorage, SQLite)
- [ ] Performance (LCP <2s, INP <200ms, CLS <0.1)
```

### **FÃ¼r Backend-APIs (Node, Python, Ruby)**
```markdown
**ZusÃ¤tzliche Bewertungskriterien:**
- [ ] API-Dokumentation (OpenAPI/Swagger)
- [ ] Rate-Limiting (Express-Rate-Limit, Flask-Limiter)
- [ ] Authentication (JWT, OAuth2, API-Keys)
- [ ] Logging (Winston, Python-Logging, Rails-Logger)
- [ ] Monitoring (Sentry, New Relic, Datadog)
```

### **FÃ¼r Data-Science-Projekte (Python, Jupyter)**
```markdown
**ZusÃ¤tzliche Bewertungskriterien:**
- [ ] Notebooks organisiert (/notebooks mit klarer Struktur)
- [ ] Requirements.txt + environment.yml (Conda)
- [ ] Data-Versioning (DVC, MLflow)
- [ ] Reproducibility (Seeds, Config-Files)
- [ ] Model-Documentation (Model-Cards)
```

---

## ğŸ“š REFERENZEN

**Benchmark-Projekt:**
- 360Volt-docu-MVP (Score: 8.5/10)
- Repo: https://github.com/M-Sieger/360Volt-docu-MVP (intern)

**Best-Practice-Quellen:**
- WCAG 2.2 Guidelines (Accessibility)
- Conventional Commits (Git-Workflow)
- 12-Factor-App (Environment, Config)
- Clean Architecture (Robert C. Martin)
- Test-Driven Development (Kent Beck)

**Tools:**
- Copilot (GitHub)
- ESLint + Prettier (Linting)
- Husky + lint-staged (Pre-Commit)
- Playwright (E2E-Tests)
- Vitest (Unit-Tests)
- Docker Compose (Environment)

---

## âœ… CHANGELOG

**v1.0 (21.10.2025):**
- Initial Release basierend auf 360Volt-Repo-Analyse
- 5 Phasen: Discovery â†’ Analysis â†’ Planning â†’ Execution â†’ Validation
- 8 Bewertungskategorien (gewichtet)
- 3-Sprint-Roadmap-Template
- Anpassungen fÃ¼r Mobile-Apps, Backend-APIs, Data-Science

**Maintainer:** GitHub Copilot  
**Review:** M. Sieger (360Volt)  
**Next Review:** Nach 5 Projekt-DurchlÃ¤ufen (Feedback-Integration)

---

## ğŸš€ QUICK START

**FÃ¼r User:**
```bash
# 1. Kopiere diesen Prompt in dein Projekt
cp /path/to/360Volt/docs/COPILOT-REPO-ANALYZER-PROMPT.md ./docs/

# 2. Ã–ffne VSCode/Codespaces

# 3. Sage zu Copilot:
"Analysiere dieses Repo nach COPILOT-REPO-ANALYZER-PROMPT.md"

# 4. Copilot fÃ¼hrt Phase 1-5 aus und fragt nach jedem Schritt: "âœ… weiter?"
```

**FÃ¼r Copilot:**
```
WENN User sagt: "Analysiere dieses Repo nach COPILOT-REPO-ANALYZER-PROMPT.md"
DANN:
1. Lese diese Datei vollstÃ¤ndig
2. Starte mit Phase 1 (Discovery)
3. Erstelle /docs/REPO-DISCOVERY-REPORT.md
4. Frage User: "âœ… weiter mit Phase 2?"
5. Wiederhole fÃ¼r Phase 2-5
6. Nach jedem Task: User-Explanation (2-3 SÃ¤tze)
7. Nach jedem Sprint: Score neu berechnen
```

---

**Ende des Prompts** ğŸ¯
